{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, W=None, b=None, random_state=None, epoch=100, lr=0.001, threshold=0.5):\n",
    "        self.random_state = random_state  # seed for data random shuffling\n",
    "        self.epoch = epoch  # n epoch = scan full data set for n times (set default as 100, but is possible to be changed by typing)\n",
    "        self.lr = lr\n",
    "        self.threshold = threshold\n",
    "        # For linked usage in terms of fit -> predict or predict_prob, set W, b as __init__.\n",
    "        self._W = W\n",
    "        self._b = b\n",
    "\n",
    "\n",
    "    def _sigmoid(self, t):  # For private usage, _sigmoid\n",
    "        return 1/(1+np.exp(-t))\n",
    "\n",
    "    def _loss(self, p, y):  # For private usage, _loss\n",
    "        return np.sum(np.power(p-y,2))  # p: value after sigmoid / x: label of item x\n",
    "\n",
    "    # Training Model\n",
    "    def fit(self,X,y):\n",
    "        # Make sure X, y are both np.array to proceed further calculation without any typeError\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        #################   parameter(W,b) initialisation   #################\n",
    "        # For initialising W(weight), b(intercept),\n",
    "        # if initial W is defined too big, output before activation(sigmoid) gets big, resulting in very small activation function slope close to 0 so it makes training speed slow.\n",
    "        # Therefore, multiply 0.01 from some random values between 0 and 1 to make initial parameters small enough.\n",
    "        # For constant value b, initialise as 0\n",
    "        self._W = np.random.uniform(0,1,size=(X.shape[1],1)) * 0.01\n",
    "        # For parameter t in sigmoid(t), t = np.dot(X,W)+ b. So, column number of X(number of features) = row number of W.\n",
    "        self._b = 0\n",
    "        #################   model training(for each epoch)   #################\n",
    "        for i in range(self.epoch):\n",
    "            xy = np.concatenate((X, y), axis = 1) # merge X, y to shuffle randomly as a whole\n",
    "            # Order items randomly\n",
    "            np.random.shuffle(xy)\n",
    "            # Split X, y again based on shuffled data\n",
    "            X = xy[:, :X.shape[1]]\n",
    "            y = xy[:,X.shape[1]].reshape(len(xy[:,X.shape[1]]),1)        # reshape to make (n,) -> (n,1)\n",
    "            # For each item x,\n",
    "            for j in range(X.shape[0]):\n",
    "                # Firstly, calculate sigmoid output for each item\n",
    "                t = np.dot(X, self._W) + self._b # [number of data, 1]\n",
    "                sig = self._sigmoid(t)  # [number of data, 1]\n",
    "                # grad based on loss function \n",
    "                grad = (sig - y) * sig * (1 - sig)  # [number of data, 1]\n",
    "                # update W, b\n",
    "                for k in range(X.shape[1]):\n",
    "                    self._W[k] = self._W[k] - self.lr * grad[j] * X[j, k]\n",
    "                self._b = self._b - self.lr * grad[j]\n",
    "            ########## This is to check whether error gets lower after every epoch ##########\n",
    "            sig_total = self._sigmoid(np.dot(X, self._W) + self._b)\n",
    "            # calculate error based on pre-defined loss function\n",
    "            error = self._loss(sig_total, y)\n",
    "            if i%10 == 0:\n",
    "                print(error)\n",
    "\n",
    "    # Predicting Test Data Set\n",
    "    def predict_probability(self, Xtest):\n",
    "        # For final W, b calculated in fit function get the predicted probability\n",
    "        prob = self._sigmoid(np.dot(Xtest, self._W) + self._b)\n",
    "        return prob\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        pred = []\n",
    "        for i in self._sigmoid(np.dot(Xtest, self._W) + self._b):\n",
    "            if i > self.threshold:\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000746939280528\n",
      "1.000690104897592\n",
      "1.000633477034454\n",
      "1.0005770617114762\n",
      "1.0005208504632082\n",
      "1.0004648327970915\n",
      "1.0004090224372375\n",
      "1.0003534015121551\n",
      "1.0002979825570202\n",
      "1.0002427682558206\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5,0.4],[0.7,0.9],[0.3,0.25],[0.6,0.76]])\n",
    "y = np.array([[0],[0],[1],[1]])\n",
    "cl = LogisticRegression(random_state=123, epoch=100)\n",
    "cl.fit(X, y)\n",
    "pred = cl.predict([0.8,0.94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.587075064697073\n",
      "0.6921697276264746\n",
      "0.37051146471186663\n",
      "0.06575681057948023\n",
      "0.07055220192570076\n",
      "0.04838112053040574\n",
      "0.035355405316907465\n",
      "0.03129714502377885\n",
      "0.028419459369182488\n",
      "0.02549907212287474\n"
     ]
    }
   ],
   "source": [
    "newX = [[160,43],[165,49],[170,58],[155,79],[163,82],[166,52],[170,100]] # This is my fake samples [height, weight]\n",
    "newY = [[0],[0],[0],[1],[1],[0],[1]]   # diabetic = 1 / non-diabetic = 0\n",
    "cl2 = LogisticRegression(epoch=100)\n",
    "cl2.fit(newX, newY)\n",
    "pred_new = cl.predict([170,88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(pred_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
